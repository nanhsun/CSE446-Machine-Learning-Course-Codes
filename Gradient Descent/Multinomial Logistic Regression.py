# -*- coding: utf-8 -*-
"""Pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pSD1eQmISQ4yoWKyeTPd4M-22Ipx8IIO
"""

import torch
from keras.datasets import mnist
import numpy as np

(X_train, labels_train), (X_test, labels_test) = mnist.load_data()
X_train = X_train.reshape(-1, 784)
X_test = X_test.reshape(-1, 784)
X_train = X_train/255.0
X_test = X_test/255.0

labels_train_hot = np.zeros([labels_train.shape[0], 10])
labels_test_hot = np.zeros([labels_train.shape[0], 10])
for count, number in enumerate(labels_train):
    labels_train_hot[count, number] = 1
for count, number in enumerate(labels_test):
    labels_test_hot[count, number] = 1

X_train = torch.from_numpy(X_train)
X_test = torch.from_numpy(X_test)
labels_train = torch.from_numpy(labels_train)
labels_test = torch.from_numpy(labels_test)

labels_train = labels_train.long()

W = torch.zeros(784, 10, dtype=torch.float64,requires_grad=True)
# W = W.double
step_size = 0.1
epochs = 1000
W_past = W + 1
W_interval = (W - W_past).detach().numpy()
# for epoch in range(epochs):
while abs(np.amax(W_interval)) >= 0.0002:
    W_past = W.clone()
    y_hat = torch.matmul(X_train, W)
    # cross entropy combines softmax calculation with NLLLoss
    loss = torch.nn.functional.cross_entropy(y_hat, labels_train)
    # computes derivatives of the loss with respect to W
    loss.backward()
    # gradient descent update
    W.data = W.data - step_size * W.grad
    # .backward() accumulates gradients into W.grad instead
    # of overwriting, so we need to zero out the weights
    W.grad.zero_()
    W_interval = (W - W_past).detach().numpy()
    print(abs(np.amax(W_interval)))

W2 = torch.zeros(784, 10, dtype=torch.float64,requires_grad=True)
step_size = 0.1
step_size = 0.1
epochs = 1000
W_past = W2 + 1
W_interval = (W2 - W_past).detach().numpy()
labels_train = labels_train.double()
while abs(np.amax(W_interval)) >= 0.001:
    W_past = W2.clone()
    y_hat = torch.matmul(W2.T, X_train.T)
    # y_hat = X_train.mul(W)
    # cross entropy combines softmax calculation with NLLLoss
    loss = torch.nn.MSELoss()
    loss = 0.5  * loss(y_hat, labels_train)
    # output = torch.nn.functional.cross_entropy(y_hat, labels_train)
    # computes derivatives of the loss with respect to W
    loss.double().backward()
    # gradient descent update
    W2.data = W2.data - step_size * W2.grad
    # .backward() accumulates gradients into W.grad instead
    # of overwriting, so we need to zero out the weights
    W2.grad.zero_()
    W_interval = (W2 - W_past).detach().numpy()
    print(abs(np.amax(W_interval)))

W = W.detach().numpy()

## classification
train_est = []
test_est = []
for i in range(X_train.shape[0]):
    temp = W.T.dot(X_train[i, :])
    train_est.append(np.where(temp == np.amax(temp))[0][0])

for i in range(X_test.shape[0]):
    temp_test = W.T.dot(X_test[i, :])
    test_est.append(np.where(temp_test == np.amax(temp_test))[0][0])

correct_train = 0
correct_test = 0
for i in range(len(train_est)):
    if train_est[i] == labels_train[i]:
        correct_train += 1
for i in range(len(test_est)):
    if test_est[i] == labels_test[i]:
        correct_test += 1

train_accuracy = correct_train/labels_train.shape[0]
test_accuracy = correct_test/labels_test.shape[0]
print("Train accuracy: ", train_accuracy)
print("Test accuracy: ", test_accuracy)

W2 = W2.detach().numpy()

train_est = []
test_est = []
for i in range(X_train.shape[0]):
    temp = W2.T.dot(X_train[i, :])
    train_est.append(np.where(temp == np.amax(temp))[0][0])

for i in range(X_test.shape[0]):
    temp_test = W2.T.dot(X_test[i, :])
    test_est.append(np.where(temp_test == np.amax(temp_test))[0][0])

correct_train = 0
correct_test = 0
for i in range(len(train_est)):
    if train_est[i] == labels_train[i]:
        correct_train += 1
for i in range(len(test_est)):
    if test_est[i] == labels_test[i]:
        correct_train += 1

train_accuracy = correct_train/labels_train.shape[0]
test_accuracy = correct_test/labels_test.shape[0]
print("Train accuracy: ", train_accuracy)
print("Test accuracy: ", test_accuracy)